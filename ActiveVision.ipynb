{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff6008e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in c:\\users\\alex\\anaconda3\\envs\\reinforcement_learning\\lib\\site-packages (9.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a211b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete \n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from stable_baselines3 import PPO, DQN, A2C, HER\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from typing import Final, Any\n",
    "import math\n",
    "from shapely.geometry.polygon import Polygon\n",
    "\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import PIL.Image as Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d8c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivVisionEnv(Env):\n",
    "    def __init__(self, num_objects: int = 3,\n",
    "                 simulation_frequency: float = 5,\n",
    "                 width: int = 5000, height: int = 2000, max_velocity_player: int = 200, view: int = 78,\n",
    "                 img_format: float = (1920 / 1080)):\n",
    "        # Actions we can take, nothing, left, right, up, down, turnCam right, turn Cam left\n",
    "        \n",
    "        \n",
    "        # consts\n",
    "        self._object_min_size: Final = 60  # in mm\n",
    "        self._object_max_size: Final = 120  # in mm\n",
    "        self._object_max_z_velocity: Final = 20  # in mm\n",
    "        self._object_max_velocity: Final = 200  # in mm/s\n",
    "        self._camera_height: Final = 390  # in mm\n",
    "        self._camera_max_angle: Final = 20  # in degree\n",
    "        self._velocity_player_per_step = 50  # max velocity change at each cycle\n",
    "        self._camera_per_step = 1  # max camera angle change at each cycle\n",
    "        self._player_max_z = width + 1\n",
    "        self._player_min_z = 1\n",
    "        self._player_pos_y = height - self._camera_height\n",
    "        self.timer = 3000\n",
    "        self._random_force_enabled = False\n",
    "        self._object_max_rnd_force_p_sec = 10 #in 1/percent from max\n",
    "        \n",
    "        # params\n",
    "        self._num_objects = num_objects\n",
    "        self._simulation_frequency = simulation_frequency\n",
    "        self._width = width\n",
    "        self._height = height\n",
    "        self._max_velocity_player = max_velocity_player\n",
    "        self._view = view\n",
    "        self._img_format = img_format\n",
    "        self._height_view = (1 / img_format) * view\n",
    "        \n",
    "        # Actions we can take, down, stay, up\n",
    "        self.action_space = Discrete(5)\n",
    "        # postition and velocities\n",
    "        self.observation_space = Box(0,1, shape=(5 + num_objects * 6,))\n",
    "        # Set start state\n",
    "        self.state = []\n",
    "        \n",
    "        # viewer\n",
    "        self.viewer = None\n",
    "        \n",
    "        for _ in range(0, 5 + 6 * num_objects):\n",
    "            self.state.append(0)\n",
    "            \n",
    "        # states for player and objects as dictionary\n",
    "        self.player = {}\n",
    "        self._objects = []\n",
    "        \n",
    "        #for drawing\n",
    "        self._last_points = 0\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def _get_normalized_state(self):\n",
    "        self.state[0] = self.player[\"pos_x\"] / self._width\n",
    "        self.state[1] = self.player[\"pos_z\"] / self._player_max_z\n",
    "        self.state[2] = (self.player[\"vel_x\"] / (2 * self._max_velocity_player)) + 0.5\n",
    "        self.state[3] = (self.player[\"vel_z\"] / (2 * self._max_velocity_player)) + 0.5\n",
    "        self.state[4] = (self.player[\"angle\"] / (2 * self._camera_max_angle)) + 0.5\n",
    "\n",
    "\n",
    "        for i in range(0, self._num_objects):\n",
    "\n",
    "            self.state[5 + 6 * i] = self._objects[i][\"pos_x\"] / self._width\n",
    "            self.state[6 + 6 * i] = self._objects[i][\"pos_y\"] / self._height\n",
    "            self.state[7 + 6 * i] = (self._objects[i][\"size\"] - self._object_min_size) / (self._object_max_size - self._object_min_size)\n",
    "            self.state[8 + 6 * i] = (self._objects[i][\"vel_x\"] / (2 * self._object_max_velocity)) + 0.5\n",
    "            self.state[9 + 6 * i] = (self._objects[i][\"vel_y\"] / (2 * self._object_max_velocity)) + 0.5\n",
    "            self.state[10 + 6 * i] = (self._objects[i][\"vel_z\"] / (2 * self._object_max_z_velocity)) + 0.5\n",
    "\n",
    "        return self.state\n",
    "        \n",
    "    def step(self, action):\n",
    "        # actions:\n",
    "        # 0 go right\n",
    "        # 1 go left\n",
    "        \n",
    "        # 2 turn cam right\n",
    "        # 3 turn cam left\n",
    "        \n",
    "        # 4 do mothing\n",
    "        \n",
    "        # --- go front\n",
    "        # --- go back\n",
    "\n",
    "        if action == 0:\n",
    "            if self.player[\"vel_x\"] < self._max_velocity_player:\n",
    "                self.player[\"vel_x\"] += self._velocity_player_per_step\n",
    "            if self.player[\"vel_x\"] > self._max_velocity_player:\n",
    "                self.player[\"vel_x\"] = self._max_velocity_player\n",
    "\n",
    "        if action == 1:\n",
    "            if self.player[\"vel_x\"] > -self._max_velocity_player:\n",
    "                self.player[\"vel_x\"] -= self._velocity_player_per_step\n",
    "            if self.player[\"vel_x\"] < -self._max_velocity_player:\n",
    "                self.player[\"vel_x\"] = -self._max_velocity_player\n",
    "        \"\"\"\n",
    "        if action == 2:\n",
    "            if self.player[\"vel_z\"] < self._max_velocity_player:\n",
    "                self.player[\"vel_z\"] += self._velocity_player_per_step\n",
    "            if self.player[\"vel_z\"] > self._max_velocity_player:\n",
    "                self.player[\"vel_z\"] = self._max_velocity_player\n",
    "\n",
    "        if action == 3:\n",
    "            if self.player[\"vel_z\"] > -self._max_velocity_player:\n",
    "                self.player[\"vel_z\"] -= self._velocity_player_per_step\n",
    "            if self.player[\"vel_z\"] < -self._max_velocity_player:\n",
    "                self.player[\"vel_z\"] = -self._max_velocity_player\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if action == 2:\n",
    "            if self.player[\"angle\"] < self._camera_max_angle:\n",
    "                self.player[\"angle\"] += self._camera_per_step\n",
    "            if self.player[\"angle\"] > self._camera_max_angle:\n",
    "                self.player[\"angle\"] = self._camera_max_angle\n",
    "\n",
    "        if action == 3:\n",
    "            if self.player[\"angle\"] > -self._camera_max_angle:\n",
    "                self.player[\"angle\"] -= self._camera_per_step\n",
    "            if self.player[\"angle\"] < -self._camera_max_angle:\n",
    "                self.player[\"angle\"] = -self._camera_max_angle\n",
    "\n",
    "\n",
    "        # calc next frame\n",
    "        self.progress_simulation()\n",
    "        reward = self._add_points()     \n",
    "        \n",
    "        \n",
    "        # Reduce timer by one step\n",
    "        self.timer -= 1 \n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = self._add_points()        \n",
    "        reward *= (1 - (abs(self.player[\"angle\"] / self._camera_max_angle)) * (1 / self._num_objects))\n",
    "        \n",
    "        # reward -= abs(self.player[\"angle\"] / self._camera_max_angle) * 0.05\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        if self.player[\"angle\"] > self._camera_per_step and action == 2:\n",
    "            reward -= (1/self._camera_max_angle+1)\n",
    "        if self.player[\"angle\"] > self._camera_per_step and action == 3:\n",
    "            reward += 0.5 * (1/self._camera_max_angle+1)\n",
    "            \n",
    "        if self.player[\"angle\"] < -self._camera_per_step and action == 3:\n",
    "            reward -= (1/self._camera_max_angle+1)\n",
    "        if self.player[\"angle\"] < -self._camera_per_step and action == 2:\n",
    "            reward += 0.5 * (1/self._camera_max_angle+1)\n",
    "        \"\"\"\n",
    "            \n",
    "        \n",
    "        \n",
    "        # Check if shower is done\n",
    "        if self.timer <= 0: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        \n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        self._get_normalized_state()\n",
    "        \n",
    "        # Return step information\n",
    "        self._last_points = reward\n",
    "        return np.asarray(self.state), reward, done, info\n",
    "\n",
    "    \n",
    "    def progress_simulation(self):\n",
    "        seconds_passed = 1 / self._simulation_frequency\n",
    "\n",
    "        # move player\n",
    "        self.player[\"pos_x\"] = self.player[\"pos_x\"] + seconds_passed * self.player[\"vel_x\"]\n",
    "        self.player[\"pos_z\"] = self.player[\"pos_z\"] + seconds_passed * self.player[\"vel_z\"]\n",
    "\n",
    "        # push in constraints\n",
    "        if self.player[\"pos_x\"] < 0:\n",
    "            self.player[\"pos_x\"] = 0\n",
    "        if self.player[\"pos_x\"] > self._width:\n",
    "            self.player[\"pos_x\"] = self._width\n",
    "        if self.player[\"pos_z\"] < self._player_min_z:\n",
    "            self.player[\"pos_z\"] = self._player_min_z\n",
    "        if self.player[\"pos_z\"] > self._player_max_z:\n",
    "            self.player[\"pos_z\"] = self._player_max_z\n",
    "\n",
    "        # move objects\n",
    "        for i in range(0, self._num_objects):\n",
    "            self._objects[i][\"pos_x\"] = self._objects[i][\"pos_x\"] + seconds_passed * self._objects[i][\"vel_x\"]\n",
    "            self._objects[i][\"pos_y\"] = self._objects[i][\"pos_y\"] + seconds_passed * self._objects[i][\"vel_y\"]\n",
    "            self._objects[i][\"size\"] = self._objects[i][\"size\"] + seconds_passed * self._objects[i][\"vel_z\"]\n",
    "\n",
    "        # collision with wall\n",
    "        for i in range(0, self._num_objects):\n",
    "            if self._objects[i][\"pos_x\"] - self._objects[i][\"size\"] / 2 <= 0:\n",
    "                self._objects[i][\"vel_x\"] = -self._objects[i][\"vel_x\"]\n",
    "                self._objects[i][\"pos_x\"] = self._objects[i][\"size\"] / 2 + 1\n",
    "                self._objects[i][\"vel_z\"] = 0\n",
    "\n",
    "            if self._objects[i][\"pos_x\"] + self._objects[i][\"size\"] / 2 >= self._width:\n",
    "                self._objects[i][\"vel_x\"] = -self._objects[i][\"vel_x\"]\n",
    "                self._objects[i][\"pos_x\"] = self._width - self._objects[i][\"size\"] / 2 - 1\n",
    "                self._objects[i][\"vel_z\"] = 0\n",
    "\n",
    "            if self._objects[i][\"pos_y\"] - self._objects[i][\"size\"] / 2 <= 0:\n",
    "                self._objects[i][\"vel_y\"] = -self._objects[i][\"vel_y\"]\n",
    "                self._objects[i][\"pos_y\"] = self._objects[i][\"size\"] / 2 + 1\n",
    "                self._objects[i][\"vel_z\"] = 0\n",
    "\n",
    "            if self._objects[i][\"pos_y\"] + self._objects[i][\"size\"] / 2 >= self._height:\n",
    "                self._objects[i][\"vel_y\"] = -self._objects[i][\"vel_y\"]\n",
    "                self._objects[i][\"pos_y\"] = self._height - self._objects[i][\"size\"] / 2 - 1\n",
    "                self._objects[i][\"vel_z\"] = 0\n",
    "\n",
    "            if self._objects[i][\"size\"] <= self._object_min_size:\n",
    "                self._objects[i][\"vel_z\"] = -self._objects[i][\"vel_z\"]\n",
    "                self._objects[i][\"size\"] = self._object_min_size + 1\n",
    "\n",
    "            if self._objects[i][\"size\"] >= self._object_max_size:\n",
    "                self._objects[i][\"vel_z\"] = -self._objects[i][\"vel_z\"]\n",
    "                self._objects[i][\"size\"] = self._object_max_size - 1\n",
    "\n",
    "        # object collision\n",
    "        for i in range(0, self._num_objects):\n",
    "            for j in range(i + 1, self._num_objects):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                polygon_i = Polygon([(self._objects[i][\"pos_x\"] - self._objects[i][\"size\"] / 2,\n",
    "                                      self._objects[i][\"pos_y\"] - self._objects[i][\"size\"] / 2),\n",
    "                                     (self._objects[i][\"pos_x\"] + self._objects[i][\"size\"] / 2,\n",
    "                                      self._objects[i][\"pos_y\"] - self._objects[i][\"size\"] / 2),\n",
    "                                     (self._objects[i][\"pos_x\"] - self._objects[i][\"size\"] / 2,\n",
    "                                      self._objects[i][\"pos_y\"] + self._objects[i][\"size\"] / 2),\n",
    "                                     (self._objects[i][\"pos_x\"] + self._objects[i][\"size\"] / 2,\n",
    "                                      self._objects[i][\"pos_y\"] + self._objects[i][\"size\"] / 2)])\n",
    "\n",
    "                polygon_j = Polygon([(self._objects[j][\"pos_x\"] - self._objects[j][\"size\"] / 2,\n",
    "                                      self._objects[j][\"pos_y\"] - self._objects[j][\"size\"] / 2),\n",
    "                                     (self._objects[j][\"pos_x\"] + self._objects[j][\"size\"] / 2,\n",
    "                                      self._objects[j][\"pos_y\"] - self._objects[j][\"size\"] / 2),\n",
    "                                     (self._objects[j][\"pos_x\"] - self._objects[j][\"size\"] / 2,\n",
    "                                      self._objects[j][\"pos_y\"] + self._objects[j][\"size\"] / 2),\n",
    "                                     (self._objects[j][\"pos_x\"] + self._objects[j][\"size\"] / 2,\n",
    "                                      self._objects[j][\"pos_y\"] + self._objects[j][\"size\"] / 2)])\n",
    "\n",
    "                if polygon_i.intersects(polygon_j):\n",
    "                    v_x = self._objects[i][\"vel_x\"]\n",
    "                    self._objects[i][\"vel_x\"] = self._objects[j][\"vel_x\"]\n",
    "                    self._objects[j][\"vel_x\"] = v_x\n",
    "\n",
    "                    v_y = self._objects[i][\"vel_y\"]\n",
    "                    self._objects[i][\"vel_y\"] = self._objects[j][\"vel_y\"]\n",
    "                    self._objects[j][\"vel_y\"] = v_y\n",
    "\n",
    "                    v_z = self._objects[i][\"vel_z\"]\n",
    "                    self._objects[i][\"vel_z\"] = self._objects[j][\"vel_z\"]\n",
    "                    self._objects[j][\"vel_z\"] = v_z\n",
    "                    \n",
    "        if self._random_force_enabled:\n",
    "            # apply force\n",
    "            for i in range(0, self._num_objects):\n",
    "                dvx = random.randint(-self._object_max_velocity, self._object_max_velocity) / \\\n",
    "                      (self._simulation_frequency * self._object_max_rnd_force_p_sec)\n",
    "                dvy = random.randint(-self._object_max_velocity, self._object_max_velocity) / \\\n",
    "                      (self._simulation_frequency * self._object_max_rnd_force_p_sec)\n",
    "                dvz = random.randint(-self._object_max_z_velocity, self._object_max_z_velocity) / \\\n",
    "                      (self._simulation_frequency * self._object_max_rnd_force_p_sec)\n",
    "\n",
    "                self._objects[i][\"vel_x\"] += dvx\n",
    "                self._objects[i][\"vel_y\"] += dvy\n",
    "                self._objects[i][\"vel_z\"] += dvz\n",
    "\n",
    "                # push it in the constraints\n",
    "                if self._objects[i][\"vel_x\"] < -self._object_max_velocity:\n",
    "                    self._objects[i][\"vel_x\"] = -self._object_max_velocity\n",
    "\n",
    "                if self._objects[i][\"vel_x\"] > self._object_max_velocity:\n",
    "                    self._objects[i][\"vel_x\"] = self._object_max_velocity\n",
    "\n",
    "                if self._objects[i][\"vel_y\"] < -self._object_max_velocity:\n",
    "                    self._objects[i][\"vel_y\"] = -self._object_max_velocity\n",
    "\n",
    "                if self._objects[i][\"vel_y\"] > self._object_max_velocity:\n",
    "                    self._objects[i][\"vel_y\"] = self._object_max_velocity\n",
    "\n",
    "                if self._objects[i][\"vel_z\"] < -self._object_max_z_velocity:\n",
    "                    self._objects[i][\"vel_z\"] = -self._object_max_z_velocity\n",
    "\n",
    "                if self._objects[i][\"vel_z\"] > self._object_max_z_velocity:\n",
    "                    self._objects[i][\"vel_z\"] = self._object_max_z_velocity\n",
    "    \n",
    "    \n",
    "    def _add_points(self):\n",
    "        corners_view = self._get_view()\n",
    "        polygon = Polygon([corners_view[\"left_top\"], corners_view[\"right_top\"],\n",
    "                           corners_view[\"right_bot\"], corners_view[\"left_bot\"]])\n",
    "\n",
    "        intersections_sum = 0\n",
    "        intersections_num = 0\n",
    "\n",
    "        area_view = polygon.area\n",
    "        for i in range(0, self._num_objects):\n",
    "            size = self._objects[i][\"size\"]\n",
    "            obj_x = self._objects[i][\"pos_x\"]\n",
    "            obj_y = self._objects[i][\"pos_y\"]\n",
    "            polygon_object = Polygon([\n",
    "                (round(obj_x - size / 2), round(obj_y - size / 2)),\n",
    "                (round(obj_x + size / 2), round(obj_y - size / 2)),\n",
    "                (round(obj_x + size / 2), round(obj_y + size / 2)),\n",
    "                (round(obj_x - size / 2), round(obj_y + size / 2))\n",
    "            ])\n",
    "            if area_view > 1:\n",
    "                intersection = (polygon.intersection(polygon_object)).area\n",
    "                if intersection > (size/4):\n",
    "                    intersections_sum += intersection\n",
    "                    intersections_num += 1\n",
    "\n",
    "        return intersections_num\n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "    def _get_view(self) -> Any:\n",
    "        \"\"\"\n",
    "        calculates the four corner points of the view of the camera in the big canvas in points in mm from top left\n",
    "        :return: dict of the four points\n",
    "        \"\"\"\n",
    "\n",
    "        # calc left , right border\n",
    "        right_border = self.player[\"pos_x\"] + self.player[\"pos_z\"] * \\\n",
    "                       math.tan((self._view / 2 - self.player[\"angle\"]) * (math.pi / 180))\n",
    "        left_border = self.player[\"pos_x\"] - self.player[\"pos_z\"] * \\\n",
    "                      math.tan((self._view / 2 + self.player[\"angle\"]) * (math.pi / 180))\n",
    "\n",
    "        # calc top and bot borders, thinking of distortion\n",
    "        left_height_diff = math.tan((self._height_view / 2) * (math.pi / 180)) * \\\n",
    "                           (self.player[\"pos_z\"] / math.sin((self._view / 2 - self.player[\"angle\"]) * (math.pi / 180)))\n",
    "\n",
    "        right_height_diff = math.tan((self._height_view / 2) * (math.pi / 180)) * \\\n",
    "                            (self.player[\"pos_z\"] / math.sin((self._view / 2 + self.player[\"angle\"]) * (math.pi / 180)))\n",
    "\n",
    "        right_top = self.player[\"pos_y\"] - right_height_diff\n",
    "        right_bot = self.player[\"pos_y\"] + right_height_diff\n",
    "        left_top = self.player[\"pos_y\"] - left_height_diff\n",
    "        left_bot = self.player[\"pos_y\"] + left_height_diff\n",
    "        \n",
    "        left_border = round(left_border)\n",
    "        left_top = round(left_top)\n",
    "        right_border = round(right_border)\n",
    "        right_top = round(right_top)\n",
    "        left_bot = round(left_bot)\n",
    "        right_bot = round(right_bot)\n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"left_top\": (left_border, left_top),\n",
    "            \"right_top\": (right_border, right_top),\n",
    "            \"left_bot\": (left_border, left_bot),\n",
    "            \"right_bot\": (right_border, right_bot)\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def render(self, mode): \n",
    "        image = Image.new(\"RGB\", (self._width, self._height))\n",
    "        draw = ImageDraw.Draw(image)\n",
    "\n",
    "        corners_view = self._get_view()\n",
    "        points = (corners_view[\"left_top\"], corners_view[\"right_top\"], corners_view[\"right_bot\"],\n",
    "                  corners_view[\"left_bot\"], corners_view[\"left_top\"])\n",
    "        \n",
    "        point_1_view = (round(self.player[\"pos_x\"]) - 20, round(self.player[\"pos_y\"]) - 20)\n",
    "        point_2_view = (round(self.player[\"pos_x\"]) + 20, round(self.player[\"pos_y\"]) + 20)\n",
    "        draw.ellipse([point_1_view, point_2_view], fill=\"white\")\n",
    "        draw.line(points, fill=\"white\", width=20)\n",
    "        colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (120, 120, 0), (0, 120, 120), (120, 0, 120)]\n",
    "        for i in range(0, self._num_objects):\n",
    "            point_1 = (round(self._objects[i][\"pos_x\"]) - 20, round(self._objects[i][\"pos_y\"]) - 20)\n",
    "            point_2 = (round(self._objects[i][\"pos_x\"]) + 20, round(self._objects[i][\"pos_y\"]) + 20)\n",
    "            draw.ellipse([point_1, point_2], fill=colors[i % len(colors)])\n",
    "            size = self._objects[i][\"size\"]\n",
    "            left_top_x = self._objects[i][\"pos_x\"]\n",
    "            left_top_y = self._objects[i][\"pos_y\"]\n",
    "\n",
    "            poly = [(round(left_top_x - size / 2), round(left_top_y - size / 2)),\n",
    "                    (round(left_top_x + size / 2), round(left_top_y - size / 2)),\n",
    "                    (round(left_top_x + size / 2), round(left_top_y + size / 2)),\n",
    "                    (round(left_top_x - size / 2), round(left_top_y + size / 2)),\n",
    "                    (round(left_top_x - size / 2), round(left_top_y - size / 2))]\n",
    "            draw.line(poly, fill=colors[i % len(colors)], width=20)\n",
    "\n",
    "        open_cv_image = np.array(image)\n",
    "        open_cv_image = open_cv_image[:, :, ::-1].copy()\n",
    "        open_cv_image = cv2.resize(open_cv_image, (900, round(640 / self._img_format)))\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        text = str(\"%.2f\" % round(self._last_points, 2))\n",
    "        cv2.putText(open_cv_image,text,(10,50), font, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(\"simulation\", open_cv_image)\n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "      \n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "    \n",
    "    def reset(self):\n",
    "        self.points = 0.0\n",
    "        self.timer = 3000\n",
    "           \n",
    "        self.player = {\n",
    "            \"pos_x\": self._width / 2 * 1.0,\n",
    "            \"pos_y\": self._height - self._camera_height * 1.0,\n",
    "            \"pos_z\": 1200.0,\n",
    "            \"vel_x\": 0.0,\n",
    "            \"vel_z\": 0.0,\n",
    "            \"angle\": 0\n",
    "        }\n",
    "        self._objects = []\n",
    "        for i in range(0, self._num_objects):\n",
    "            dict_to_append = {\n",
    "                \"pos_x\": (self._width / 2) + (i - math.floor(self._num_objects / 2)) * self._object_max_size,\n",
    "                \"pos_y\": (self._height / 2) + i * self._object_max_size,\n",
    "                \"vel_x\": random.randint(-self._object_max_velocity, self._object_max_velocity) * 0.5,\n",
    "                \"vel_y\": random.randint(-self._object_max_velocity, self._object_max_velocity) * 0.3,\n",
    "                \"vel_z\": random.randint(-self._object_max_z_velocity, self._object_max_z_velocity) * 0.3,\n",
    "                \"size\": random.randint(self._object_min_size, self._object_max_size) * 1.0\n",
    "            }\n",
    "            self._objects.append(dict_to_append)\n",
    "            \n",
    "        self._get_normalized_state()\n",
    "        return np.asarray(self.state, dtype=np.float32)\n",
    "    \n",
    "        \n",
    "        # state:\n",
    "        # 0 pos player x\n",
    "        # 1 pos player z\n",
    "        # 2 vel player x\n",
    "        # 3 vel player z\n",
    "        # 4 angle\n",
    "\n",
    "        # 5 pos obj 1 x\n",
    "        # 6 pos obj 1 y\n",
    "        # 7 pos obj 1 z\n",
    "        # 8 vel obj 1 x\n",
    "        # 9 vel obj 1 y\n",
    "        # 10 vel obj 1 z\n",
    "        # the 5-10 in repeat for every object\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15bf5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=ActivVisionEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7777914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38226798, 0.53483295, 0.25878283, 0.01773131, 0.99714977,\n",
       "       0.13238326, 0.10229607, 0.4958193 , 0.3336368 , 0.26561204,\n",
       "       0.9914827 , 0.22469492, 0.36096227, 0.53730017, 0.9778157 ,\n",
       "       0.75629646, 0.0514673 , 0.6150094 , 0.02086553, 0.13781987,\n",
       "       0.11339293, 0.8398807 , 0.6960859 ], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe6e883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.23995201, 0.5       , 0.5       , 0.5       ,\n",
       "       0.476     , 0.5       , 0.2       , 0.4125    , 0.35075   ,\n",
       "       0.4775    , 0.5       , 0.56      , 0.78333336, 0.46625   ,\n",
       "       0.45875   , 0.365     , 0.524     , 0.62      , 0.48333332,\n",
       "       0.69125   , 0.37625   , 0.395     ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bb351f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7224686 , 0.10055874, 0.09142943, 0.7147802 , 0.26198882,\n",
       "       0.42549172, 0.05038687, 0.4537625 , 0.9451006 , 0.0349783 ,\n",
       "       0.6177249 , 0.8583874 , 0.29039767, 0.24891557, 0.08741511,\n",
       "       0.45354038, 0.84802467, 0.9595081 , 0.7236449 , 0.3884699 ,\n",
       "       0.6316933 , 0.80885744, 0.39915746], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Box(0,1, shape=(5 + 3 * 6,)).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a6506c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "344c0b77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "171b7e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:2387.100000000004\n",
      "Episode:2 Score:2017.7999999999997\n",
      "Episode:3 Score:1529.266666666685\n",
      "Episode:4 Score:1317.4333333333348\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m----> 8\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     10\u001b[0m     n_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mActivVisionEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    385\u001b[0m     poly \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mround\u001b[39m(left_top_x \u001b[38;5;241m-\u001b[39m size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mround\u001b[39m(left_top_y \u001b[38;5;241m-\u001b[39m size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m    386\u001b[0m             (\u001b[38;5;28mround\u001b[39m(left_top_x \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mround\u001b[39m(left_top_y \u001b[38;5;241m-\u001b[39m size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m    387\u001b[0m             (\u001b[38;5;28mround\u001b[39m(left_top_x \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mround\u001b[39m(left_top_y \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m    388\u001b[0m             (\u001b[38;5;28mround\u001b[39m(left_top_x \u001b[38;5;241m-\u001b[39m size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mround\u001b[39m(left_top_y \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m    389\u001b[0m             (\u001b[38;5;28mround\u001b[39m(left_top_x \u001b[38;5;241m-\u001b[39m size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mround\u001b[39m(left_top_y \u001b[38;5;241m-\u001b[39m size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m))]\n\u001b[0;32m    390\u001b[0m     draw\u001b[38;5;241m.\u001b[39mline(poly, fill\u001b[38;5;241m=\u001b[39mcolors[i \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(colors)], width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m--> 392\u001b[0m open_cv_image \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m open_cv_image \u001b[38;5;241m=\u001b[39m open_cv_image[:, :, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    394\u001b[0m open_cv_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(open_cv_image, (\u001b[38;5;241m900\u001b[39m, \u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m640\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_img_format)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render(\"mode\")\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0d7d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f863700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f24dde46",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m DQN(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[43mlog_path\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'log_path' is not defined"
     ]
    }
   ],
   "source": [
    "model = DQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f88b7529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\DQN_8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1500000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reinforcement_learning\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:258\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    247\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m     reset_num_timesteps: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OffPolicyAlgorithm:\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDQN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reinforcement_learning\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:354\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    351\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 354\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reinforcement_learning\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:587\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    584\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[0;32m    586\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m--> 587\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    590\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reinforcement_learning\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reinforcement_learning\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 43\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reinforcement_learning\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:90\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mActivVisionEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# calc next frame\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_simulation()\n\u001b[1;32m--> 130\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m     \n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Reduce timer by one step\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mActivVisionEnv._add_points\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    310\u001b[0m polygon_object \u001b[38;5;241m=\u001b[39m Polygon([\n\u001b[0;32m    311\u001b[0m     (\u001b[38;5;28mround\u001b[39m(obj_x \u001b[38;5;241m-\u001b[39m size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mround\u001b[39m(obj_y \u001b[38;5;241m-\u001b[39m size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m    312\u001b[0m     (\u001b[38;5;28mround\u001b[39m(obj_x \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mround\u001b[39m(obj_y \u001b[38;5;241m-\u001b[39m size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m    313\u001b[0m     (\u001b[38;5;28mround\u001b[39m(obj_x \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mround\u001b[39m(obj_y \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m    314\u001b[0m     (\u001b[38;5;28mround\u001b[39m(obj_x \u001b[38;5;241m-\u001b[39m size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mround\u001b[39m(obj_y \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m    315\u001b[0m ])\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m area_view \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     intersection \u001b[38;5;241m=\u001b[39m (\u001b[43mpolygon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintersection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolygon_object\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39marea\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m intersection \u001b[38;5;241m>\u001b[39m (size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m    319\u001b[0m         intersections_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m intersection\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reinforcement_learning\\lib\\site-packages\\shapely\\geometry\\base.py:689\u001b[0m, in \u001b[0;36mBaseGeometry.intersection\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mintersection\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the intersection of the geometries\"\"\"\u001b[39;00m\n\u001b[1;32m--> 689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m geom_factory(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mintersection\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reinforcement_learning\\lib\\site-packages\\shapely\\topology.py:68\u001b[0m, in \u001b[0;36mBinaryTopologicalOp.__call__\u001b[1;34m(self, this, other, *args)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate(this)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate(other, stop_prepared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 68\u001b[0m product \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_geom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_geom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m product \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     err \u001b[38;5;241m=\u001b[39m TopologicalError(\n\u001b[0;32m     71\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis operation could not be performed. Reason: unknown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=1500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66842bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path = os.path.join('Training', 'Saved Models', 'DQN_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed5b2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21f32988",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "635f731b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = DQN.load(PPO_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0527bf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\anaconda3\\envs\\reinforcement_learning\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reinforcement_learning\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:123\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m    120\u001b[0m                 current_lengths[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[1;32m--> 123\u001b[0m         \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m mean_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(episode_rewards)\n\u001b[0;32m    126\u001b[0m std_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(episode_rewards)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reinforcement_learning\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:85\u001b[0m, in \u001b[0;36mDummyVecEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03mGym environment rendering. If there are multiple environments then\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03mthey are tiled together in one image via ``BaseVecEnv.render()``.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m:param mode: The rendering type.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrender(mode\u001b[38;5;241m=\u001b[39mmode)\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mActivVisionEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    393\u001b[0m     draw\u001b[38;5;241m.\u001b[39mline(poly, fill\u001b[38;5;241m=\u001b[39mcolors[i \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(colors)], width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m    395\u001b[0m open_cv_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image)\n\u001b[1;32m--> 396\u001b[0m open_cv_image \u001b[38;5;241m=\u001b[39m \u001b[43mopen_cv_image\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m open_cv_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(open_cv_image, (\u001b[38;5;241m900\u001b[39m, \u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m640\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_img_format)))\n\u001b[0;32m    398\u001b[0m font \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=1, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34ae5b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env=ActivVisionEnv()\n",
    "\n",
    "model = PPO.load(PPO_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe066ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
